import boto3
import logging
import os
import pandas as pd
import re
import tempfile
import io
import csv

from collections import defaultdict
from datetime import datetime
from typing import Optional, List, Any, Tuple
from usaspending_api import settings

from usaspending_api.common.helpers.timing_helpers import Timer


logger = logging.getLogger("script")


def get_deleted_records(start_datetime: datetime, end_datetime: Optional[datetime] = None) -> dict:
    """
    Connect to S3 and gather all of the transaction ids stored in CSV files
    generated by the broker when transactions are removed from the DB.

    Return a dictionary of lists. Keys are dates and the value are lists of IDs removed on that date.
    IDs are a mix of procurement (FPDS) and assistance (FABS)
    """

    logger.info("Gathering all deleted transactions from S3")

    bucket_objects = access_s3_object_list(settings.DELETED_TRANSACTION_JOURNAL_FILES)

    logger.info(f"CSV data from {start_datetime} to {end_datetime if end_datetime else 'now'}")

    filtered_csv_list = [x for x in bucket_objects if (x.key.endswith(".csv") and not x.key.startswith("staging"))]

    logger.info(f"Found {len(filtered_csv_list)} csv files")

    delete_events = {}

    for obj in filtered_csv_list:
        # Use temporary files to facilitate date moving from csv files on S3 into pands
        bucket = get_s3_bucket(bucket_name=settings.DELETED_TRANSACTION_JOURNAL_FILES)
        (file, file_path) = tempfile.mkstemp()
        bucket.download_file(obj.key, file_path)

        # Ingests the CSV into a dataframe. pandas thinks some ids are dates, so disable parsing
        data = pd.read_csv(file_path, dtype=str)
        event_date = obj.last_modified

        if "detached_award_proc_unique" in data:
            new_ids = ["CONT_TX_" + x.upper() for x in data["detached_award_proc_unique"].values]
        elif "afa_generated_unique" in data:
            new_ids = ["ASST_TX_" + x.upper() for x in data["afa_generated_unique"].values]
        else:
            logger.info(f"  [Missing valid col] in {obj.key}")

        # Next statements are ugly, but properly handle the temp files
        os.close(file)
        os.remove(file_path)

        if event_date in delete_events:
            delete_events[event_date].append(new_ids)
        else:
            delete_events[event_date] = new_ids

    logger.info(f"Total delete dates: {len(delete_events)}")

    # datetime.strptime(item[: item.find("_")], "%m-%d-%Y").date()
    for key in delete_events:
        if key <= start_datetime:
            del delete_events[key]
        if end_datetime and key >= end_datetime:
            del delete_events[key]

    logger.info(f"Returning {len(delete_events)} delete dates after filtering for desired range")
    return delete_events

    #     for uid in new_ids:
    #         if uid in deleted_ids:
    #             if deleted_ids[uid]["timestamp"] < obj.last_modified:
    #                 deleted_ids[uid]["timestamp"] = obj.last_modified
    #         else:
    #             deleted_ids[uid] = {"timestamp": obj.last_modified}

    # for uid, deleted_dict in deleted_ids.items():
    #     logger.info("id: {} last modified: {}".format(uid, str(deleted_dict["timestamp"])))

    # logger.info(f"Gathering {len(deleted_ids)} deleted transactions took {perf_counter() - start}s")
    # return deleted_ids


def retrieve_deleted_fabs_transactions(start_datetime: datetime, end_datetime: Optional[datetime] = None) -> dict:
    FABS_regex = ".*_FABSdeletions_.*"
    date_format = "%Y-%m-%d"
    with Timer("Obtaining S3 Object list"):
        objects = access_s3_object_list(settings.DELETED_TRANSACTION_JOURNAL_FILES, regex_pattern=FABS_regex)
        objects = limit_objects_to_date_range(objects, date_format, start_datetime, end_datetime)

    deleted_records = defaultdict(list)
    for obj in objects:
        object_data, object_file = access_s3_object(settings.DELETED_TRANSACTION_JOURNAL_FILES, obj)
        with open(object_file, "r") as f:
            reader = csv.reader(f.read().splitlines())
            next(reader)

            transaction_id_list = [rows[0] for rows in reader]
            if transaction_id_list:
                file_date = obj.key[: obj.key.find("_")]
                deleted_records[file_date].extend(transaction_id_list)

    return deleted_records


def retrieve_deleted_fpds_transactions(start_datetime: datetime, end_datetime: Optional[datetime] = None) -> dict:
    FPDS_regex = ".*_delete_records_(IDV|award).*"
    date_format = "%m-%d-%Y"
    with Timer("Obtaining S3 Object list"):
        objects = access_s3_object_list(settings.DELETED_TRANSACTION_JOURNAL_FILES, regex_pattern=FPDS_regex)
        objects = limit_objects_to_date_range(objects, date_format, start_datetime, end_datetime)

    deleted_records = defaultdict(list)
    for obj in objects:
        object_data, object_file = access_s3_object(settings.DELETED_TRANSACTION_JOURNAL_FILES, obj)
        with open(object_file, "r") as f:
            reader = csv.reader(f.read().splitlines())
            next(reader)

            transaction_id_list = [rows[0] for rows in reader]
            if transaction_id_list:
                file_date = obj.key[: obj.key.find("_")]
                deleted_records[file_date].extend(transaction_id_list)

    return deleted_records


def limit_objects_to_date_range(
    objects: List[Any], date_format: str, start_datetime: datetime, end_datetime: Optional[datetime] = None
):
    results = []
    for obj in objects:
        file_date = datetime.strptime(obj.key[: obj.key.find("_")], date_format)
        # Only keep S3 objects which fall between the provided dates
        if file_date < start_datetime:
            continue
        if end_datetime and file_date >= end_datetime:
            continue
        results.append(obj)

    return results


def access_s3_object_list(bucket_name: str, regex_pattern: Optional[str] = None) -> List[Any]:
    """Find all S3 objects in provided bucket.

    If regex is passed, only keys which match the regex are returned
    """

    logger.info("Gathering all deleted transactions from S3")
    try:
        bucket = get_s3_bucket(bucket_name=bucket_name)
        bucket_objects = list(bucket.objects.all())
    except Exception:
        msg = (
            "Verify settings.USASPENDING_AWS_REGION and settings.DELETED_TRANSACTION_JOURNAL_FILES are correct "
            "or env variables: USASPENDING_AWS_REGION and DELETED_TRANSACTION_JOURNAL_FILES are set"
        )
        logger.exception(msg)
        bucket_objects = None

    if regex_pattern:
        bucket_objects = [obj for obj in bucket_objects if re.search(regex_pattern, obj.key)]

    return bucket_objects


def get_s3_bucket(bucket_name: str, region_name: str = settings.USASPENDING_AWS_REGION) -> boto3.resource:
    s3 = boto3.resource("s3", region_name=region_name)
    return s3.Bucket(bucket_name)


def access_s3_object(bucket_name: str, obj: Any) -> Tuple[io.TextIOWrapper, str]:
    """"""
    bucket = get_s3_bucket(bucket_name=bucket_name)
    (file, file_path) = tempfile.mkstemp()
    bucket.download_file(obj.key, file_path)
    with open(file, "r") as f:
        return f, file_path
